{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#**1.  Data Cleaning / Data Preprocessing**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CH4K9W6L_E_R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAko8Tv4-4iV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import skew, kurtosis\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration des graphiques\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "sns.set_palette(\"husl\")"
      ],
      "metadata": {
        "id": "qfgI4WBx--2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Chemin relatif depuis le dossier racine du projet\n",
        "df = pd.read_csv(\"/content/noisy_paddydataset.csv\")\n",
        "\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"APERÇU GÉNÉRAL DU DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Dimensions du dataset: {df.shape}\")\n",
        "print(f\"Nombre total de cellules: {df.shape[0] * df.shape[1]:,}\")\n",
        "print(df.head())\n",
        "print(\"=\"*60)\n",
        "print(\"APERÇU GÉNÉRAL DU DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Dimensions du dataset: {df.shape}\")\n",
        "print(f\"Nombre total de cellules: {df.shape[0] * df.shape[1]:,}\")"
      ],
      "metadata": {
        "id": "ZTg0owAr_OF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Informations générales\n",
        "print(\"INFORMATIONS GÉNÉRALES:\")\n",
        "print(\"-\" * 30)\n",
        "df.info()\n",
        "print()"
      ],
      "metadata": {
        "id": "TAMXSJDK_TNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistiques descriptives de base\n",
        "print(\"STATISTIQUES DESCRIPTIVES - VARIABLES NUMÉRIQUES:\")\n",
        "print(\"-\" * 50)\n",
        "print(df.describe())"
      ],
      "metadata": {
        "id": "4bivkjWv_VJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 3. DISTINCTION VARIABLES NUMÉRIQUES ET CATÉGORIQUES\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"DISTINCTION VARIABLES NUMÉRIQUES ET CATÉGORIQUES\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Identification des types de variables\n",
        "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"Variables numériques: {len(numeric_features)}\")\n",
        "print(f\"  Exemples: {numeric_features[:10]}\")\n",
        "print(f\"\\nVariables catégoriques: {len(categorical_features)}\")\n",
        "print(f\"  Exemples: {categorical_features[:10]}\")"
      ],
      "metadata": {
        "id": "VfZo-hc5_hGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# 2. ANALYSE DES DONNÉES MANQUANTES\n",
        "# ================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANALYSE DES DONNÉES MANQUANTES\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "vkVt0VEh_h-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul des valeurs manquantes\n",
        "missing_data = df.isnull().sum()\n",
        "missing_percent = (missing_data / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Colonnes': missing_data.index,\n",
        "    'Valeurs_Manquantes': missing_data.values,\n",
        "    'Pourcentage': missing_percent.values\n",
        "}).sort_values('Pourcentage', ascending=False)"
      ],
      "metadata": {
        "id": "htP2i3WZ_qVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Affichage des colonnes avec des valeurs manquantes\n",
        "missing_cols = missing_df[missing_df['Valeurs_Manquantes'] > 0]\n",
        "print(f\"Nombre de colonnes avec des valeurs manquantes: {len(missing_cols)}\")\n",
        "print(\"\\nColonnes avec le plus de valeurs manquantes:\")\n",
        "print(missing_cols.head(10))"
      ],
      "metadata": {
        "id": "MaA6STLr_rVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation des données manquantes\n",
        "plt.figure(figsize=(15, 8))\n",
        "missing_cols_top = missing_cols.head(15)\n",
        "plt.barh(missing_cols_top['Colonnes'], missing_cols_top['Pourcentage'])\n",
        "plt.xlabel('Pourcentage de valeurs manquantes')\n",
        "plt.title('Top 15 des colonnes avec des valeurs manquantes')\n",
        "plt.gca().invert_yaxis()\n"
      ],
      "metadata": {
        "id": "hDFv2dcz_whe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Heatmap des valeurs manquantes (échantillon)\n",
        "sample_cols = missing_cols.head(10)['Colonnes'].tolist()\n",
        "if sample_cols:\n",
        "    sns.heatmap(df[sample_cols].isnull(), cbar=True, yticklabels=False, cmap='viridis')\n",
        "    plt.title('Heatmap des valeurs manquantes\\n(Top 10 colonnes)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2QNt3Mk7_1Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 Visualisation : Histogrammes avec Mean & Median"
      ],
      "metadata": {
        "id": "hVQk9P40AP_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer une grille d'histogrammes (max 3 par ligne)\n",
        "n_cols = 3\n",
        "n_rows = (len(numeric_features) + n_cols - 1) // n_cols\n",
        "\n",
        "plt.figure(figsize=(20, 5 * n_rows))\n",
        "\n",
        "for i, col in enumerate(numeric_features, 1):\n",
        "    plt.subplot(n_rows, n_cols, i)\n",
        "    data = df[col].dropna()\n",
        "\n",
        "    sns.histplot(data, kde=True, stat=\"density\", alpha=0.7, color='skyblue', linewidth=0)\n",
        "\n",
        "    mean_val = data.mean()\n",
        "    median_val = data.median()\n",
        "\n",
        "    plt.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Moyenne: {mean_val:.2f}')\n",
        "    plt.axvline(median_val, color='green', linestyle='-', linewidth=2, label=f'Médiane: {median_val:.2f}')\n",
        "\n",
        "    skewness = stats.skew(data)\n",
        "    plt.title(f'{col}\\\\nSkewness: {skewness:.2f}', fontsize=12)\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Densité')\n",
        "    plt.legend(fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Distributions des Variables Numériques (avec Moyenne & Médiane)', fontsize=16, y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "An8U1tUP_5fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 Tests Statistiques & Suggestion d'Imputation"
      ],
      "metadata": {
        "id": "pcAlImo-AZbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse statistique pour chaque colonne numérique\n",
        "imputation_suggestions = []\n",
        "\n",
        "for col in numeric_features:\n",
        "    data = df[col].dropna()\n",
        "    n = len(data)\n",
        "\n",
        "    if n < 3:\n",
        "        continue  # Pas assez de données pour les tests\n",
        "\n",
        "    mean = data.mean()\n",
        "    median = data.median()\n",
        "    skewness = stats.skew(data)\n",
        "    kurt = stats.kurtosis(data)\n",
        "\n",
        "    # Test de normalité selon taille d'échantillon\n",
        "    if n <= 500:\n",
        "        # Shapiro-Wilk (p-value > 0.05 → normal)\n",
        "        shapiro_stat, shapiro_p = stats.shapiro(data)\n",
        "        test_name = 'Shapiro-Wilk'\n",
        "        p_value = shapiro_p\n",
        "    elif n <= 5000:\n",
        "        # Kolmogorov-Smirnov (données standardisées)\n",
        "        standardized_data = (data - mean) / data.std()\n",
        "        ks_stat, ks_p = stats.kstest(standardized_data, 'norm')\n",
        "        test_name = 'Kolmogorov-Smirnov'\n",
        "        p_value = ks_p\n",
        "    else:\n",
        "        # Trop grand dataset → on se fie à skewness\n",
        "        p_value = np.nan\n",
        "        test_name = 'N/A'\n",
        "\n",
        "    # Décision d'imputation\n",
        "    if abs(skewness) <= 0.5 and (np.isnan(p_value) or p_value > 0.05):\n",
        "        imputation = \"Moyenne (distribution symétrique et normale)\"\n",
        "    else:\n",
        "        imputation = \"Médiane (distribution asymétrique ou non normale)\"\n",
        "\n",
        "    imputation_suggestions.append({\n",
        "        'Colonne': col,\n",
        "        'Moyenne': round(mean, 2),\n",
        "        'Médiane': round(median, 2),\n",
        "        'Skewness': round(skewness, 3),\n",
        "        'Test Normalité': test_name,\n",
        "        'p-value': round(p_value, 4) if not np.isnan(p_value) else 'N/A',\n",
        "        'Suggestion Imputation': imputation\n",
        "    })\n",
        "\n",
        "# Affichage des résultats\n",
        "results_df = pd.DataFrame(imputation_suggestions)\n",
        "results_df = results_df.sort_values(by='Skewness', key=abs, ascending=False)\n",
        "\n",
        "print(\"RÉSUMÉ STATISTIQUE ET SUGGESTIONS D'IMPUTATION\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.to_string(index=False))"
      ],
      "metadata": {
        "id": "MyGosuFyAaos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3 Application de l'Imputation Recommandée"
      ],
      "metadata": {
        "id": "8Nis1ZUSAliD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned = df.copy()\n",
        "# Imputation automatique selon les suggestions\n",
        "for col in numeric_features:\n",
        "    data = df_cleaned[col]\n",
        "    if data.isnull().sum() == 0:\n",
        "        continue  # Rien à imputer\n",
        "\n",
        "    # Récupérer la suggestion\n",
        "    suggestion = results_df.loc[results_df['Colonne'] == col, 'Suggestion Imputation'].values[0]\n",
        "\n",
        "    if \"Moyenne\" in suggestion:\n",
        "        df_cleaned[col].fillna(df_cleaned[col].mean(), inplace=True)\n",
        "        print(f\"{col} : imputé par la MOYENNE\")\n",
        "    else:\n",
        "        df_cleaned[col].fillna(df_cleaned[col].median(), inplace=True)\n",
        "        print(f\"{col} : imputé par la MÉDIANE\")\n",
        "\n",
        "# Vérification finale\n",
        "print(f\"\\nValeurs manquantes restantes (numériques) : {df_cleaned[numeric_features].isnull().sum().sum()}\")"
      ],
      "metadata": {
        "id": "mh7PcTJgAmeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.4 Détection des Outliers"
      ],
      "metadata": {
        "id": "HQenE_-lAu9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Après imputation des valeurs aberrantes\n",
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "def auto_treat_outliers(df, numeric_features, min_rows_threshold=5000, plot=True):\n",
        "    \"\"\"\n",
        "    Traite automatiquement les outliers en choisissant la méthode la plus adéquate.\n",
        "\n",
        "    Critères de décision :\n",
        "    - Si très peu d'outliers (<1%) et dataset grand → Suppression possible\n",
        "    - Si skewness > 1 et valeurs >=0 → Transformation log\n",
        "    - Si % outliers > 5% → Capping percentiles 5%/95%\n",
        "    - Sinon → Robust IQR + remplacement par médiane\n",
        "    \"\"\"\n",
        "\n",
        "    summary = []\n",
        "    rows_to_drop = set()\n",
        "    initial_rows = len(df)\n",
        "\n",
        "    # Sauvegarde des données AVANT traitement (pour visualisation)\n",
        "    df_before = df.copy()\n",
        "\n",
        "    print(\"TRAITEMENT AUTOMATIQUE DES OUTLIERS - CHOIX INTELLIGENT PAR COLONNE\")\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "    # ============================\n",
        "    # TRAITEMENT COLONNE PAR COLONNE\n",
        "    # ============================\n",
        "    for col in numeric_features:\n",
        "        data = df[col].dropna()\n",
        "\n",
        "        if len(data) < 10:\n",
        "            summary.append({\n",
        "                'Colonne': col,\n",
        "                'Méthode Choisie': 'Ignoré (trop peu de données)'\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # Statistiques\n",
        "        skewness = stats.skew(data)\n",
        "        median = data.median()\n",
        "\n",
        "        # Détection IQR\n",
        "        Q1 = data.quantile(0.25)\n",
        "        Q3 = data.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower = Q1 - 1.5 * IQR\n",
        "        upper = Q3 + 1.5 * IQR\n",
        "\n",
        "        outlier_mask = (df[col] < lower) | (df[col] > upper)\n",
        "        nb_outliers = outlier_mask.sum()\n",
        "        percent_outliers = nb_outliers / len(df) * 100\n",
        "\n",
        "        method_chosen = \"\"\n",
        "        action = \"\"\n",
        "\n",
        "        # ============================\n",
        "        # DÉCISION AUTOMATIQUE\n",
        "        # ============================\n",
        "        if nb_outliers == 0:\n",
        "            method_chosen = \"Aucun outlier\"\n",
        "            action = \"Rien à faire\"\n",
        "\n",
        "        elif percent_outliers < 1 and len(df) > min_rows_threshold:\n",
        "            rows_to_drop.update(df.loc[outlier_mask].index)\n",
        "            method_chosen = \"Suppression\"\n",
        "            action = f\"{nb_outliers} lignes supprimées\"\n",
        "\n",
        "        elif abs(skewness) > 1 and data.min() >= 0:\n",
        "            df[col + \"_log\"] = np.log1p(df[col])\n",
        "            method_chosen = \"Transformation log\"\n",
        "            action = \"Nouvelle colonne _log créée\"\n",
        "\n",
        "        elif percent_outliers > 5:\n",
        "            df[col] = winsorize(df[col], limits=[0.05, 0.05])\n",
        "            method_chosen = \"Capping (5%/95%)\"\n",
        "            action = \"Winsorization appliquée\"\n",
        "\n",
        "        else:\n",
        "            df.loc[outlier_mask, col] = median\n",
        "            method_chosen = \"Robust IQR + médiane\"\n",
        "            action = f\"{nb_outliers} outliers remplacés par médiane\"\n",
        "\n",
        "        summary.append({\n",
        "            'Colonne': col,\n",
        "            'Skewness': round(skewness, 3),\n",
        "            '% Outliers': round(percent_outliers, 2),\n",
        "            'Méthode Choisie': method_chosen,\n",
        "            'Action': action\n",
        "        })\n",
        "\n",
        "    # ============================\n",
        "    # SUPPRESSION FINALE (UNE FOIS)\n",
        "    # ============================\n",
        "    if rows_to_drop:\n",
        "        df.drop(index=list(rows_to_drop), inplace=True)\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "        df_before = df_before.loc[df.index].reset_index(drop=True)\n",
        "\n",
        "    # ============================\n",
        "    # VISUALISATION AVANT / APRÈS\n",
        "    # ============================\n",
        "    if plot:\n",
        "        fig, axes = plt.subplots(\n",
        "            nrows=len(numeric_features),\n",
        "            ncols=2,\n",
        "            figsize=(14, 4 * len(numeric_features))\n",
        "        )\n",
        "\n",
        "        if len(numeric_features) == 1:\n",
        "            axes = np.array([axes])\n",
        "\n",
        "        for i, col in enumerate(numeric_features):\n",
        "            sns.boxplot(\n",
        "                y=df_before[col],\n",
        "                ax=axes[i, 0],\n",
        "                color='lightcoral'\n",
        "            )\n",
        "            axes[i, 0].set_title(f'Avant - {col}')\n",
        "\n",
        "            sns.boxplot(\n",
        "                y=df[col],\n",
        "                ax=axes[i, 1],\n",
        "                color='lightgreen'\n",
        "            )\n",
        "            axes[i, 1].set_title(f'Après - {col}')\n",
        "\n",
        "        plt.suptitle(\"Comparaison AVANT / APRÈS par colonne\", fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # ============================\n",
        "    # RÉCAPITULATIF\n",
        "    # ============================\n",
        "    final_rows = len(df)\n",
        "    print(f\"\\nNombre de lignes : {initial_rows} → {final_rows}\")\n",
        "\n",
        "    summary_df = pd.DataFrame(summary)\n",
        "    print(\"\\nRÉCAPITULATIF DU TRAITEMENT AUTOMATIQUE\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "\n",
        "    return df, summary_df"
      ],
      "metadata": {
        "id": "_DsNsn4UAv56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned, recap = auto_treat_outliers(df_cleaned, numeric_features, plot=True)\n",
        "\n",
        "\n",
        "print(\"\\n=== DATAFRAME APRÈS TRAITEMENT ===\")\n",
        "display(df_cleaned)"
      ],
      "metadata": {
        "id": "hwuDPnrqA3sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== RÉCAPITULATIF ===\")\n",
        "display(recap)"
      ],
      "metadata": {
        "id": "J9rxsUDXBFbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 2.5 Imputation + Uniformisation des Variables Catégorielles\n",
        "# ===================================================================\n",
        "\n",
        "print(\"ÉTAT AVANT TRAITEMENT\")\n",
        "print(\"=\"*80)\n",
        "for col in categorical_features:\n",
        "    if col in df_cleaned.columns:\n",
        "        missing = df_cleaned[col].isnull().sum()\n",
        "        uniques = df_cleaned[col].nunique()\n",
        "        print(f\"{col:30} → {missing:3} valeurs manquantes | {uniques} valeurs uniques\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAITEMENT : IMPUTATION (mode) + UNIFORMISATION (minuscules + strip)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "treatment_summary = []\n",
        "\n",
        "for col in categorical_features:\n",
        "    if col not in df_cleaned.columns:\n",
        "        print(f\"{col} : colonne non présente\")\n",
        "        continue\n",
        "\n",
        "    initial_missing = df_cleaned[col].isnull().sum()\n",
        "    initial_unique = df_cleaned[col].nunique()\n",
        "\n",
        "    # 1. Imputation des valeurs manquantes par le mode\n",
        "    if initial_missing > 0:\n",
        "        mode_value = df_cleaned[col].mode(dropna=True)[0]\n",
        "        df_cleaned[col].fillna(mode_value, inplace=True)\n",
        "        imputed = True\n",
        "    else:\n",
        "        mode_value = None\n",
        "        imputed = False\n",
        "\n",
        "    # 2. Uniformisation : minuscules + suppression espaces\n",
        "    df_cleaned[col] = df_cleaned[col].astype(str).str.lower().str.strip()\n",
        "\n",
        "    # Après traitement\n",
        "    final_missing = df_cleaned[col].isnull().sum()\n",
        "    final_unique = df_cleaned[col].nunique()\n",
        "\n",
        "    print(f\"✓ {col}\")\n",
        "    if imputed:\n",
        "        print(f\"   → {initial_missing} valeurs manquantes imputées par mode : '{mode_value}'\")\n",
        "    print(f\"   → Valeurs uniques : {initial_unique} → {final_unique} (réduction des doublons de casse)\")\n",
        "\n",
        "    treatment_summary.append({\n",
        "        'Colonne': col,\n",
        "        'Manquantes Initiales': initial_missing,\n",
        "        'Imputé par Mode': mode_value if imputed else 'Non',\n",
        "        'Uniques Avant': initial_unique,\n",
        "        'Uniques Après': final_unique\n",
        "    })\n",
        "\n",
        "# Vérification finale\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"VÉRIFICATION FINALE\")\n",
        "print(\"=\"*80)\n",
        "total_missing_cat = df_cleaned[categorical_features].isnull().sum().sum()\n",
        "print(f\"Valeurs manquantes restantes (catégorielles) : {total_missing_cat} → doit être 0\")\n",
        "\n",
        "print(\"\\nValeurs uniques finales par colonne :\")\n",
        "for col in categorical_features:\n",
        "    if col in df_cleaned.columns:\n",
        "        print(f\"   {col:30} → {df_cleaned[col].nunique()} valeurs : {sorted(df_cleaned[col].unique())[:10]}...\")\n",
        "\n",
        "# Tableau récapitulatif\n",
        "summary_df = pd.DataFrame(treatment_summary)\n",
        "print(\"\\nRÉCAPITULATIF DU TRAITEMENT\")\n",
        "print(summary_df.to_string(index=False))"
      ],
      "metadata": {
        "id": "slSdm4lVI6v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer le dossier data s'il n'existe pas\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "# Enregistrement en CSV\n",
        "df_cleaned.to_csv(\"data/cleaned_paddydataset.csv\", index=False)\n",
        "\n",
        "print(\"\\nDataFrame sauvegardé dans : data/cleaned_paddydataset.csv\")"
      ],
      "metadata": {
        "id": "w4fXX2gGBan9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}