{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmcS7ayXcSRd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import skew, kurtosis\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "import joblib\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration des graphiques\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "sns.set_palette(\"husl\")"
      ],
      "metadata": {
        "id": "zH84g-4Ncfy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le fichier original\n",
        "df = pd.read_csv('data/paddy_dataset_fe.csv', sep=',', encoding='utf-8', low_memory=False)\n",
        "# Création d'une copie pour ne pas modifier df original\n",
        "df_reg = df.copy()"
      ],
      "metadata": {
        "id": "1UcO9pPhcw46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aperçu des premières lignes\n",
        "print(\"APERÇU DES DONNÉES:\")\n",
        "print(\"-\" * 30)\n",
        "print(df_reg.head())\n",
        "print()"
      ],
      "metadata": {
        "id": "WrnbYRaoc4JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Target\n",
        "target = 'Paddy yield(in Kg)'\n",
        "y = df[target]\n",
        "X = df_reg.drop(target, axis=1)\n",
        "\n",
        "print(f\"Nombre total de features avant sélection : {X.shape[1]}\")\n",
        "\n",
        "# ========================================\n",
        "# 1. Feature Selection avec SelectKBest (univarié)\n",
        "# ========================================\n",
        "# On garde les 12 meilleures features selon f_regression\n",
        "selector = SelectKBest(score_func=f_regression, k=12)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Récupérer les noms des features sélectionnées\n",
        "selected_features = X.columns[selector.get_support()].tolist()\n",
        "\n",
        "print(f\"Features sélectionnées ({len(selected_features)}) :\")\n",
        "print(selected_features)\n",
        "\n",
        "# Nouveau DataFrame avec seulement les features sélectionnées + cible\n",
        "df_selected = df_reg[selected_features + [target]]\n",
        "\n",
        "# ========================================\n",
        "# 2. Train-Test Split\n",
        "# ========================================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_selected.drop(target, axis=1),\n",
        "    df_selected[target],\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ========================================\n",
        "# 3. Scaling\n",
        "# ========================================\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"\\nTrain set : {X_train.shape}\")\n",
        "print(f\"Test set  : {X_test.shape}\")\n",
        "print(\"Données prêtes pour la modélisation !\")"
      ],
      "metadata": {
        "id": "xMiaTERgdGIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MODÉLISATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Dictionnaire pour stocker les résultats\n",
        "results = {}\n",
        "\n",
        "def evaluate_model(name, model, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Entraîne et évalue un modèle\n",
        "    \"\"\"\n",
        "    # Entraînement\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Prédictions\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # Métriques sur l'ensemble d'entraînement\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "    train_r2 = r2_score(y_train, y_train_pred)\n",
        "\n",
        "    # Métriques sur l'ensemble de test\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "    test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "    # Validation croisée\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5,\n",
        "                                scoring='neg_root_mean_squared_error')\n",
        "    cv_rmse = -cv_scores.mean()\n",
        "\n",
        "    return {\n",
        "        'Model': name,\n",
        "        'Train_RMSE': train_rmse,\n",
        "        'Train_MAE': train_mae,\n",
        "        'Train_R2': train_r2,\n",
        "        'Test_RMSE': test_rmse,\n",
        "        'Test_MAE': test_mae,\n",
        "        'Test_R2': test_r2,\n",
        "        'CV_RMSE': cv_rmse,\n",
        "        'Predictions_Train': y_train_pred,\n",
        "        'Predictions_Test': y_test_pred\n",
        "    }"
      ],
      "metadata": {
        "id": "fZMoF7qqdlu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 1. LINEAR REGRESSION\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n1. LINEAR REGRESSION\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "lr_model = LinearRegression()\n",
        "results['Linear Regression'] = evaluate_model('Linear Regression', lr_model,\n",
        "                                              X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "print(f\"  Train RMSE: {results['Linear Regression']['Train_RMSE']:.4f}\")\n",
        "print(f\"  Test RMSE:  {results['Linear Regression']['Test_RMSE']:.4f}\")\n",
        "print(f\"  Train MAE:  {results['Linear Regression']['Train_MAE']:.4f}\")\n",
        "print(f\"  Test MAE:   {results['Linear Regression']['Test_MAE']:.4f}\")\n",
        "print(f\"  Train R²:   {results['Linear Regression']['Train_R2']:.4f}\")\n",
        "print(f\"  Test R²:    {results['Linear Regression']['Test_R2']:.4f}\")\n",
        "print(f\"  CV RMSE:    {results['Linear Regression']['CV_RMSE']:.4f}\")"
      ],
      "metadata": {
        "id": "rsSltgfTdqX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 2. LASSO REGRESSION\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n2. LASSO REGRESSION\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "lasso_model = Lasso(alpha=0.001, max_iter=10000, random_state=42)\n",
        "results['Lasso'] = evaluate_model('Lasso', lasso_model,\n",
        "                                 X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "print(f\"  Train RMSE: {results['Lasso']['Train_RMSE']:.4f}\")\n",
        "print(f\"  Test RMSE:  {results['Lasso']['Test_RMSE']:.4f}\")\n",
        "print(f\"  Train MAE:  {results['Lasso']['Train_MAE']:.4f}\")\n",
        "print(f\"  Test MAE:   {results['Lasso']['Test_MAE']:.4f}\")\n",
        "print(f\"  Train R²:   {results['Lasso']['Train_R2']:.4f}\")\n",
        "print(f\"  Test R²:    {results['Lasso']['Test_R2']:.4f}\")\n",
        "print(f\"  CV RMSE:    {results['Lasso']['CV_RMSE']:.4f}\")\n",
        "\n",
        "# Nombre de coefficients non-nuls\n",
        "non_zero_coef = np.sum(lasso_model.coef_ != 0)\n",
        "print(f\"  Coefficients non-nuls: {non_zero_coef}/{len(lasso_model.coef_)}\")"
      ],
      "metadata": {
        "id": "GAWB2c9vdutn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 3. RIDGE REGRESSION\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n3. RIDGE REGRESSION\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "ridge_model = Ridge(alpha=10, random_state=42)\n",
        "results['Ridge'] = evaluate_model('Ridge', ridge_model,\n",
        "                                 X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "print(f\"  Train RMSE: {results['Ridge']['Train_RMSE']:.4f}\")\n",
        "print(f\"  Test RMSE:  {results['Ridge']['Test_RMSE']:.4f}\")\n",
        "print(f\"  Train MAE:  {results['Ridge']['Train_MAE']:.4f}\")\n",
        "print(f\"  Test MAE:   {results['Ridge']['Test_MAE']:.4f}\")\n",
        "print(f\"  Train R²:   {results['Ridge']['Train_R2']:.4f}\")\n",
        "print(f\"  Test R²:    {results['Ridge']['Test_R2']:.4f}\")\n",
        "print(f\"  CV RMSE:    {results['Ridge']['CV_RMSE']:.4f}\")"
      ],
      "metadata": {
        "id": "JmUWaFMedxHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 4. ELASTIC NET\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n4. ELASTIC NET\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "elasticnet_model = ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=10000,\n",
        "                             random_state=42)\n",
        "results['ElasticNet'] = evaluate_model('ElasticNet', elasticnet_model,\n",
        "                                      X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "print(f\"  Train RMSE: {results['ElasticNet']['Train_RMSE']:.4f}\")\n",
        "print(f\"  Test RMSE:  {results['ElasticNet']['Test_RMSE']:.4f}\")\n",
        "print(f\"  Train MAE:  {results['ElasticNet']['Train_MAE']:.4f}\")\n",
        "print(f\"  Test MAE:   {results['ElasticNet']['Test_MAE']:.4f}\")\n",
        "print(f\"  Train R²:   {results['ElasticNet']['Train_R2']:.4f}\")\n",
        "print(f\"  Test R²:    {results['ElasticNet']['Test_R2']:.4f}\")\n",
        "print(f\"  CV RMSE:    {results['ElasticNet']['CV_RMSE']:.4f}\")\n",
        "\n",
        "non_zero_coef_en = np.sum(elasticnet_model.coef_ != 0)\n",
        "print(f\"  Coefficients non-nuls: {non_zero_coef_en}/{len(elasticnet_model.coef_)}\")"
      ],
      "metadata": {
        "id": "_jkFkEmEdznb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 5. XGBOOST REGRESSOR\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "print(\"\\n5. XGBOOST REGRESSOR\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Initialisation du modèle\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    n_estimators=500,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Entraînement et évaluation\n",
        "results['XGBoost'] = evaluate_model('XGBoost', xgb_model,\n",
        "                                    X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "# Affichage des résultats\n",
        "print(f\"  Train RMSE: {results['XGBoost']['Train_RMSE']:.4f}\")\n",
        "print(f\"  Test RMSE:  {results['XGBoost']['Test_RMSE']:.4f}\")\n",
        "print(f\"  Train MAE:  {results['XGBoost']['Train_MAE']:.4f}\")\n",
        "print(f\"  Test MAE:   {results['XGBoost']['Test_MAE']:.4f}\")\n",
        "print(f\"  Train R²:   {results['XGBoost']['Train_R2']:.4f}\")\n",
        "print(f\"  Test R²:    {results['XGBoost']['Test_R2']:.4f}\")\n",
        "print(f\"  CV RMSE:    {results['XGBoost']['CV_RMSE']:.4f}\")"
      ],
      "metadata": {
        "id": "6KcVw1rLd2q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# COMPARAISON DES MODÈLES\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"COMPARAISON DES MODÈLES\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Modèle': list(results.keys()),\n",
        "    'Train_RMSE': [results[m]['Train_RMSE'] for m in results.keys()],\n",
        "    'Test_RMSE': [results[m]['Test_RMSE'] for m in results.keys()],\n",
        "    'Train_MAE': [results[m]['Train_MAE'] for m in results.keys()],\n",
        "    'Test_MAE': [results[m]['Test_MAE'] for m in results.keys()],\n",
        "    'Train_R2': [results[m]['Train_R2'] for m in results.keys()],\n",
        "    'Test_R2': [results[m]['Test_R2'] for m in results.keys()],\n",
        "    'CV_RMSE': [results[m]['CV_RMSE'] for m in results.keys()]\n",
        "})\n",
        "\n",
        "print(\"\\nTABLEAU COMPARATIF:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualisation de la comparaison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "models = list(results.keys())\n",
        "x_pos = np.arange(len(models))\n",
        "\n",
        "# RMSE Comparison\n",
        "axes[0, 0].bar(x_pos - 0.2, comparison_df['Train_RMSE'], 0.4,\n",
        "              label='Train RMSE', color='skyblue')\n",
        "axes[0, 0].bar(x_pos + 0.2, comparison_df['Test_RMSE'], 0.4,\n",
        "              label='Test RMSE', color='lightcoral')\n",
        "axes[0, 0].set_xticks(x_pos)\n",
        "axes[0, 0].set_xticklabels(models, rotation=45, ha='right')\n",
        "axes[0, 0].set_ylabel('RMSE')\n",
        "axes[0, 0].set_title('Comparaison RMSE (plus bas = meilleur)')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# MAE Comparison\n",
        "axes[0, 1].bar(x_pos - 0.2, comparison_df['Train_MAE'], 0.4,\n",
        "              label='Train MAE', color='lightgreen')\n",
        "axes[0, 1].bar(x_pos + 0.2, comparison_df['Test_MAE'], 0.4,\n",
        "              label='Test MAE', color='gold')\n",
        "axes[0, 1].set_xticks(x_pos)\n",
        "axes[0, 1].set_xticklabels(models, rotation=45, ha='right')\n",
        "axes[0, 1].set_ylabel('MAE')\n",
        "axes[0, 1].set_title('Comparaison MAE (plus bas = meilleur)')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# R² Comparison\n",
        "axes[1, 0].bar(x_pos - 0.2, comparison_df['Train_R2'], 0.4,\n",
        "              label='Train R²', color='mediumpurple')\n",
        "axes[1, 0].bar(x_pos + 0.2, comparison_df['Test_R2'], 0.4,\n",
        "              label='Test R²', color='orange')\n",
        "axes[1, 0].set_xticks(x_pos)\n",
        "axes[1, 0].set_xticklabels(models, rotation=45, ha='right')\n",
        "axes[1, 0].set_ylabel('R²')\n",
        "axes[1, 0].set_title('Comparaison R² (plus haut = meilleur)')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "axes[1, 0].set_ylim([min(comparison_df['Test_R2'])-0.05, 1.0])\n",
        "\n",
        "# Overfitting analysis (différence Train-Test)\n",
        "overfit_rmse = comparison_df['Train_RMSE'] - comparison_df['Test_RMSE']\n",
        "axes[1, 1].bar(x_pos, overfit_rmse, color='salmon')\n",
        "axes[1, 1].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
        "axes[1, 1].set_xticks(x_pos)\n",
        "axes[1, 1].set_xticklabels(models, rotation=45, ha='right')\n",
        "axes[1, 1].set_ylabel('Train RMSE - Test RMSE')\n",
        "axes[1, 1].set_title('Analyse Overfitting\\n(valeurs négatives = bon)')\n",
        "axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Prédictions vs Valeurs réelles\n",
        "n_models = len(models)\n",
        "\n",
        "fig, axes = plt.subplots(\n",
        "    n_models, 1, figsize=(8, 4 * n_models)\n",
        ")\n",
        "\n",
        "# Cas où il n’y a qu’un seul modèle\n",
        "if n_models == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i, model_name in enumerate(models):\n",
        "    y_pred = results[model_name]['Predictions_Test']\n",
        "\n",
        "    axes[i].scatter(y_test, y_pred, alpha=0.5)\n",
        "    axes[i].plot(\n",
        "        [y_test.min(), y_test.max()],\n",
        "        [y_test.min(), y_test.max()],\n",
        "        'r--', lw=2, label='Parfait'\n",
        "    )\n",
        "    axes[i].set_xlabel('Valeurs réelles')\n",
        "    axes[i].set_ylabel('Prédictions')\n",
        "    axes[i].set_title(\n",
        "        f'{model_name} | R² test = {results[model_name][\"Test_R2\"]:.4f}'\n",
        "    )\n",
        "    axes[i].legend()\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RXjxbmSld9Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Ridge semble être le modèle le plus robuste et le plus fiable.`\n",
        "\n"
      ],
      "metadata": {
        "id": "tLTQpZdxeDGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# III. SÉLECTION DE VARIABLES - BACKWARD ELIMINATION\n",
        "# ================================================================\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*70)\n",
        "print(\"PARTIE III: SÉLECTION DE VARIABLES PAR BACKWARD ELIMINATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"BACKWARD ELIMINATION\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "def backward_elimination(X, y, significance_level=0.05):\n",
        "    \"\"\"\n",
        "    Effectue une élimination backward basée sur les p-values\n",
        "\n",
        "    Paramètres:\n",
        "    -----------\n",
        "    X : DataFrame\n",
        "        Features\n",
        "    y : Series\n",
        "        Target\n",
        "    significance_level : float\n",
        "        Seuil de significativité (défaut: 0.05)\n",
        "\n",
        "    Retourne:\n",
        "    ---------\n",
        "    selected_features : list\n",
        "        Liste des features sélectionnées\n",
        "    \"\"\"\n",
        "    features = list(X.columns)\n",
        "    removed_features = []\n",
        "\n",
        "    print(f\"\\nDébut: {len(features)} variables\")\n",
        "    print(f\"Seuil de significativité: {significance_level}\")\n",
        "\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        iteration += 1\n",
        "\n",
        "        # Ajouter une constante\n",
        "        X_with_const = sm.add_constant(X[features])\n",
        "\n",
        "        # Ajuster le modèle\n",
        "        model = sm.OLS(y, X_with_const).fit()\n",
        "\n",
        "        # Trouver la variable avec la p-value la plus élevée\n",
        "        p_values = model.pvalues.iloc[1:]  # Exclure la constante\n",
        "        max_p_value = p_values.max()\n",
        "\n",
        "        if max_p_value > significance_level:\n",
        "            excluded_feature = p_values.idxmax()\n",
        "            features.remove(excluded_feature)\n",
        "            removed_features.append((excluded_feature, max_p_value))\n",
        "\n",
        "            if iteration <= 10 or iteration % 10 == 0:\n",
        "                print(f\"Itération {iteration}: Retrait de '{excluded_feature}' (p-value={max_p_value:.4f})\")\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    print(f\"\\nTerminé après {iteration} itérations\")\n",
        "    print(f\"Variables conservées: {len(features)}\")\n",
        "    print(f\"Variables retirées: {len(removed_features)}\")\n",
        "\n",
        "    return features, removed_features\n",
        "\n",
        "# Appliquer backward elimination\n",
        "selected_features, removed_features = backward_elimination(X_train, y_train,\n",
        "                                                          significance_level=0.05)\n",
        "\n",
        "print(f\"\\n{len(selected_features)} variables sélectionnées sur {X_train.shape[1]}\")\n",
        "print(f\"Réduction: {(1 - len(selected_features)/X_train.shape[1])*100:.1f}%\")\n",
        "\n",
        "# Afficher quelques variables retirées\n",
        "print(\"\\nPremières variables retirées (Top 10 p-values):\")\n",
        "removed_sorted = sorted(removed_features, key=lambda x: x[1], reverse=True)\n",
        "for feat, pval in removed_sorted[:10]:\n",
        "    print(f\"  • {feat}: p-value = {pval:.6f}\")\n",
        "\n",
        "# Préparer les nouveaux ensembles train/test avec les variables sélectionnées\n",
        "X_train_selected = X_train[selected_features]\n",
        "X_test_selected = X_test[selected_features]\n",
        "\n",
        "print(f\"\\nNouvelles dimensions:\")\n",
        "print(f\"  X_train: {X_train_selected.shape}\")"
      ],
      "metadata": {
        "id": "eaCRaW8AeMLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# BACKWARD ELIMINATION\n",
        "# ===========================\n",
        "selected_features, removed_features = backward_elimination(X_train, y_train,\n",
        "                                                          significance_level=0.05)\n",
        "\n",
        "print(f\"\\n{len(selected_features)} variables sélectionnées sur {X_train.shape[1]}\")\n",
        "print(f\"Réduction: {(1 - len(selected_features)/X_train.shape[1])*100:.1f}%\")\n",
        "\n",
        "# Afficher les 10 premières variables retirées\n",
        "removed_sorted = sorted(removed_features, key=lambda x: x[1], reverse=True)\n",
        "print(\"\\nPremières variables retirées (Top 10 p-values):\")\n",
        "for feat, pval in removed_sorted[:10]:\n",
        "    print(f\"  • {feat}: p-value = {pval:.6f}\")\n",
        "\n",
        "# ===========================\n",
        "# Préparer les ensembles train/test sélectionnés\n",
        "# ===========================\n",
        "X_train_selected = X_train[selected_features]\n",
        "X_test_selected = X_test[selected_features]\n",
        "\n",
        "print(f\"\\nNouvelles dimensions après sélection:\")\n",
        "print(f\"  X_train: {X_train_selected.shape}\")\n",
        "print(f\"  X_test : {X_test_selected.shape}\")\n",
        "\n",
        "# ===========================\n",
        "# Appliquer le scaling seulement sur les features sélectionnées\n",
        "# ===========================\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "print(\"\\nDonnées prêtes pour la modélisation avec features sélectionnées !\")"
      ],
      "metadata": {
        "id": "KFUQ737xeRbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principe de fonctionnement\n",
        "\n",
        "```\n",
        "Backward Elimination\n",
        "\n",
        "1. Entraîner un modèle avec toutes les variables\n",
        "\n",
        "2. Identifier la variable la moins significative\n",
        "\n",
        "3. La supprimer\n",
        "\n",
        "4. Réentraîner le modèle\n",
        "\n",
        "5. Répéter jusqu’à un critère d’arrêt\n",
        "\n",
        "SelectKBest\n",
        "\n",
        "1. Calculer un score pour chaque variable\n",
        "\n",
        "2. Trier les variables par score\n",
        "\n",
        "3. Sélectionner les K meilleures\n",
        "\n",
        "4. Pas de ré-entraînement du modèle pendant la sélection\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "7_ARKg3ueWrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 1. LINEAR REGRESSION\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n1. LINEAR REGRESSION\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "lr_model = LinearRegression()\n",
        "results['Linear Regression'] = evaluate_model('Linear Regression', lr_model,\n",
        "                                              X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "print(f\"  Train RMSE: {results['Linear Regression']['Train_RMSE']:.4f}\")\n",
        "print(f\"  Test RMSE:  {results['Linear Regression']['Test_RMSE']:.4f}\")\n",
        "print(f\"  Train MAE:  {results['Linear Regression']['Train_MAE']:.4f}\")\n",
        "print(f\"  Test MAE:   {results['Linear Regression']['Test_MAE']:.4f}\")\n",
        "print(f\"  Train R²:   {results['Linear Regression']['Train_R2']:.4f}\")\n",
        "print(f\"  Test R²:    {results['Linear Regression']['Test_R2']:.4f}\")"
      ],
      "metadata": {
        "id": "PPg2V9CXemx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 2. LASSO REGRESSION\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n2. LASSO REGRESSION\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "lasso_model = Lasso(alpha=0.001, max_iter=10000, random_state=42)\n",
        "results['Lasso'] = evaluate_model('Lasso', lasso_model,\n",
        "                                 X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "print(f\"  Train RMSE: {results['Lasso']['Train_RMSE']:.4f}\")\n",
        "print(f\"  Test RMSE:  {results['Lasso']['Test_RMSE']:.4f}\")\n",
        "print(f\"  Train MAE:  {results['Lasso']['Train_MAE']:.4f}\")\n",
        "print(f\"  Test MAE:   {results['Lasso']['Test_MAE']:.4f}\")\n",
        "print(f\"  Train R²:   {results['Lasso']['Train_R2']:.4f}\")\n",
        "print(f\"  Test R²:    {results['Lasso']['Test_R2']:.4f}\")\n",
        "print(f\"  CV RMSE:    {results['Lasso']['CV_RMSE']:.4f}\")\n",
        "\n",
        "# Nombre de coefficients non-nuls\n",
        "non_zero_coef = np.sum(lasso_model.coef_ != 0)\n",
        "print(f\"  Coefficients non-nuls: {non_zero_coef}/{len(lasso_model.coef_)}\")"
      ],
      "metadata": {
        "id": "so4fMzdcepHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 3. RIDGE REGRESSION\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n3. RIDGE REGRESSION\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "ridge_model = Ridge(alpha=10, random_state=42)\n",
        "results['Ridge'] = evaluate_model('Ridge', ridge_model,\n",
        "                                 X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "print(f\"  Train RMSE: {results['Ridge']['Train_RMSE']:.4f}\")\n",
        "print(f\"  Test RMSE:  {results['Ridge']['Test_RMSE']:.4f}\")\n",
        "print(f\"  Train MAE:  {results['Ridge']['Train_MAE']:.4f}\")\n",
        "print(f\"  Test MAE:   {results['Ridge']['Test_MAE']:.4f}\")\n",
        "print(f\"  Train R²:   {results['Ridge']['Train_R2']:.4f}\")\n",
        "print(f\"  Test R²:    {results['Ridge']['Test_R2']:.4f}\")\n",
        "print(f\"  CV RMSE:    {results['Ridge']['CV_RMSE']:.4f}\")"
      ],
      "metadata": {
        "id": "9aVrDd41eri4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 4. ELASTIC NET\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n4. ELASTIC NET\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "elasticnet_model = ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=10000,\n",
        "                             random_state=42)\n",
        "results['ElasticNet'] = evaluate_model('ElasticNet', elasticnet_model,\n",
        "                                      X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "print(f\"  Train RMSE: {results['ElasticNet']['Train_RMSE']:.4f}\")\n",
        "print(f\"  Test RMSE:  {results['ElasticNet']['Test_RMSE']:.4f}\")\n",
        "print(f\"  Train MAE:  {results['ElasticNet']['Train_MAE']:.4f}\")\n",
        "print(f\"  Test MAE:   {results['ElasticNet']['Test_MAE']:.4f}\")\n",
        "print(f\"  Train R²:   {results['ElasticNet']['Train_R2']:.4f}\")\n",
        "print(f\"  Test R²:    {results['ElasticNet']['Test_R2']:.4f}\")\n",
        "print(f\"  CV RMSE:    {results['ElasticNet']['CV_RMSE']:.4f}\")\n",
        "\n",
        "non_zero_coef_en = np.sum(elasticnet_model.coef_ != 0)\n",
        "print(f\"  Coefficients non-nuls: {non_zero_coef_en}/{len(elasticnet_model.coef_)}\")"
      ],
      "metadata": {
        "id": "i9oE8D8leuLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 5. XGBOOST REGRESSOR\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "print(\"\\n5. XGBOOST REGRESSOR\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Initialisation du modèle\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    n_estimators=500,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Entraînement et évaluation\n",
        "results['XGBoost'] = evaluate_model('XGBoost', xgb_model,\n",
        "                                    X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "# Affichage des résultats\n",
        "print(f\"  Train RMSE: {results['XGBoost']['Train_RMSE']:.4f}\")\n",
        "print(f\"  Test RMSE:  {results['XGBoost']['Test_RMSE']:.4f}\")\n",
        "print(f\"  Train MAE:  {results['XGBoost']['Train_MAE']:.4f}\")\n",
        "print(f\"  Test MAE:   {results['XGBoost']['Test_MAE']:.4f}\")\n",
        "print(f\"  Train R²:   {results['XGBoost']['Train_R2']:.4f}\")\n",
        "print(f\"  Test R²:    {results['XGBoost']['Test_R2']:.4f}\")\n",
        "print(f\"  CV RMSE:    {results['XGBoost']['CV_RMSE']:.4f}\")"
      ],
      "metadata": {
        "id": "3EviqVAbew7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# COMPARAISON DES MODÈLES\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"COMPARAISON DES MODÈLES\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Modèle': list(results.keys()),\n",
        "    'Train_RMSE': [results[m]['Train_RMSE'] for m in results.keys()],\n",
        "    'Test_RMSE': [results[m]['Test_RMSE'] for m in results.keys()],\n",
        "    'Train_MAE': [results[m]['Train_MAE'] for m in results.keys()],\n",
        "    'Test_MAE': [results[m]['Test_MAE'] for m in results.keys()],\n",
        "    'Train_R2': [results[m]['Train_R2'] for m in results.keys()],\n",
        "    'Test_R2': [results[m]['Test_R2'] for m in results.keys()],\n",
        "    'CV_RMSE': [results[m]['CV_RMSE'] for m in results.keys()],\n",
        "})\n",
        "\n",
        "print(\"\\nTABLEAU COMPARATIF:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualisation de la comparaison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "models = list(results.keys())\n",
        "x_pos = np.arange(len(models))\n",
        "\n",
        "# RMSE Comparison\n",
        "axes[0, 0].bar(x_pos - 0.2, comparison_df['Train_RMSE'], 0.4,\n",
        "              label='Train RMSE', color='skyblue')\n",
        "axes[0, 0].bar(x_pos + 0.2, comparison_df['Test_RMSE'], 0.4,\n",
        "              label='Test RMSE', color='lightcoral')\n",
        "axes[0, 0].set_xticks(x_pos)\n",
        "axes[0, 0].set_xticklabels(models, rotation=45, ha='right')\n",
        "axes[0, 0].set_ylabel('RMSE')\n",
        "axes[0, 0].set_title('Comparaison RMSE (plus bas = meilleur)')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# MAE Comparison\n",
        "axes[0, 1].bar(x_pos - 0.2, comparison_df['Train_MAE'], 0.4,\n",
        "              label='Train MAE', color='lightgreen')\n",
        "axes[0, 1].bar(x_pos + 0.2, comparison_df['Test_MAE'], 0.4,\n",
        "              label='Test MAE', color='gold')\n",
        "axes[0, 1].set_xticks(x_pos)\n",
        "axes[0, 1].set_xticklabels(models, rotation=45, ha='right')\n",
        "axes[0, 1].set_ylabel('MAE')\n",
        "axes[0, 1].set_title('Comparaison MAE (plus bas = meilleur)')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# R² Comparison\n",
        "axes[1, 0].bar(x_pos - 0.2, comparison_df['Train_R2'], 0.4,\n",
        "              label='Train R²', color='mediumpurple')\n",
        "axes[1, 0].bar(x_pos + 0.2, comparison_df['Test_R2'], 0.4,\n",
        "              label='Test R²', color='orange')\n",
        "axes[1, 0].set_xticks(x_pos)\n",
        "axes[1, 0].set_xticklabels(models, rotation=45, ha='right')\n",
        "axes[1, 0].set_ylabel('R²')\n",
        "axes[1, 0].set_title('Comparaison R² (plus haut = meilleur)')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "axes[1, 0].set_ylim([min(comparison_df['Test_R2'])-0.05, 1.0])\n",
        "\n",
        "# Overfitting analysis (différence Train-Test)\n",
        "overfit_rmse = comparison_df['Train_RMSE'] - comparison_df['Test_RMSE']\n",
        "axes[1, 1].bar(x_pos, overfit_rmse, color='salmon')\n",
        "axes[1, 1].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
        "axes[1, 1].set_xticks(x_pos)\n",
        "axes[1, 1].set_xticklabels(models, rotation=45, ha='right')\n",
        "axes[1, 1].set_ylabel('Train RMSE - Test RMSE')\n",
        "axes[1, 1].set_title('Analyse Overfitting\\n(valeurs négatives = bon)')\n",
        "axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Prédictions vs Valeurs réelles\n",
        "n_models = len(models)\n",
        "\n",
        "fig, axes = plt.subplots(\n",
        "    n_models, 1, figsize=(8, 4 * n_models)\n",
        ")\n",
        "\n",
        "# Cas où il n’y a qu’un seul modèle\n",
        "if n_models == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i, model_name in enumerate(models):\n",
        "    y_pred = results[model_name]['Predictions_Test']\n",
        "\n",
        "    axes[i].scatter(y_test, y_pred, alpha=0.5)\n",
        "    axes[i].plot(\n",
        "        [y_test.min(), y_test.max()],\n",
        "        [y_test.min(), y_test.max()],\n",
        "        'r--', lw=2, label='Parfait'\n",
        "    )\n",
        "    axes[i].set_xlabel('Valeurs réelles')\n",
        "    axes[i].set_ylabel('Prédictions')\n",
        "    axes[i].set_title(\n",
        "        f'{model_name} | R² test = {results[model_name][\"Test_R2\"]:.4f}'\n",
        "    )\n",
        "    axes[i].legend()\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fpqlQSpve2BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Test RMSE (erreur quadratique) :\tXGBoost → 2938.82 (plus bas)\n",
        "- Test MAE (erreur absolue)\t: XGBoost → 1550.29 (plus bas)\n",
        "- Test R² (variance expliquée) :\tXGBoost → 0.88295 (plus haut)\n",
        "\n",
        "`Bien que la régression linéaire, Lasso, Ridge et ElasticNet aient des performances correctes (R² ~0.875–0.876), XGBoost surpasse tous les modèles sur toutes les métriques test.`\n"
      ],
      "metadata": {
        "id": "eM9NxlzXfNNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- SelectKBest → pour filtrer rapidement un petit nombre de features importantes.\n",
        "\n",
        "- Backward Elimination → pour une sélection précise et interprétable, mais plus lente.\n",
        "\n",
        "- GridSearchCV → pour optimiser le modèle final après avoir choisi les features."
      ],
      "metadata": {
        "id": "lMW4XBrVgJfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GridSearchCV**"
      ],
      "metadata": {
        "id": "1rghTkiN88ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1. Définir les modèles optimisés (après GridSearchCV)\n",
        "# ------------------------------------------------------------\n",
        "# ------------------------------------------------------------\n",
        "# 1. Définir les modèles optimisés (après GridSearchCV)\n",
        "# ------------------------------------------------------------\n",
        "optimized_models = {\n",
        "    \"Ridge Optimisé\": Ridge(alpha=10, random_state=42),\n",
        "    \"Lasso Optimisé\": Lasso(alpha=0.0001, max_iter=10000, random_state=42),\n",
        "    \"ElasticNet Optimisé\": ElasticNet(alpha=0.1, l1_ratio=0.9, max_iter=10000, random_state=42),\n",
        "    \"XGBoost Optimisé\": XGBRegressor(\n",
        "        n_estimators=500,\n",
        "        max_depth=5,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. Fonction pour entraîner et évaluer un modèle\n",
        "# ------------------------------------------------------------\n",
        "def evaluate_model(name, model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # Metrics\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "    train_r2 = r2_score(y_train, y_train_pred)\n",
        "    test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "    # CV RMSE (XGBoost peut lancer un warning avec cross_val_score si eval_metric)\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
        "    cv_rmse = -cv_scores.mean()\n",
        "\n",
        "    return {\n",
        "        'Model': name,\n",
        "        'Train_RMSE': train_rmse,\n",
        "        'Test_RMSE': test_rmse,\n",
        "        'Train_MAE': train_mae,\n",
        "        'Test_MAE': test_mae,\n",
        "        'Train_R2': train_r2,\n",
        "        'Test_R2': test_r2,\n",
        "        'CV_RMSE': cv_rmse\n",
        "    }\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. Évaluer tous les modèles optimisés\n",
        "# ------------------------------------------------------------\n",
        "results_optimized = {}\n",
        "for name, model in optimized_models.items():\n",
        "    results_optimized[name] = evaluate_model(name, model, X_train_selected, X_test_selected, y_train, y_test)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4. Créer le tableau comparatif final\n",
        "# ------------------------------------------------------------\n",
        "comparison_df_optimized = pd.DataFrame({\n",
        "    'Modèle': [results_optimized[m]['Model'] for m in results_optimized],\n",
        "    'Train_RMSE': [results_optimized[m]['Train_RMSE'] for m in results_optimized],\n",
        "    'Test_RMSE': [results_optimized[m]['Test_RMSE'] for m in results_optimized],\n",
        "    'Train_MAE': [results_optimized[m]['Train_MAE'] for m in results_optimized],\n",
        "    'Test_MAE': [results_optimized[m]['Test_MAE'] for m in results_optimized],\n",
        "    'Train_R2': [results_optimized[m]['Train_R2'] for m in results_optimized],\n",
        "    'Test_R2': [results_optimized[m]['Test_R2'] for m in results_optimized],\n",
        "    'CV_RMSE': [results_optimized[m]['CV_RMSE'] for m in results_optimized],\n",
        "})\n",
        "\n",
        "print(\"\\nTABLEAU COMPARATIF DES MODÈLES OPTIMISÉS:\")\n",
        "print(comparison_df_optimized.to_string(index=False))"
      ],
      "metadata": {
        "id": "iVIO14JbgdGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "1. XGBoost Optimisé est le meilleur modèle globalement :\n",
        "\n",
        "- Plus faible RMSE et MAE\n",
        "\n",
        "- Meilleur R² (train et test)\n",
        "\n",
        "- Captures non-linéarités et interactions que les modèles linéaires n’expliquent pas\n",
        "\n",
        "2. Les modèles linéaires (Ridge, Lasso, ElasticNet) sont proches entre eux et se comportent très bien pour des relations linéaires, mais ils sont légèrement moins performants que XGBoost sur ce dataset.\n",
        "```"
      ],
      "metadata": {
        "id": "wMktzjlAhKZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer le dossier si nécessaire\n",
        "os.makedirs(\"regression\", exist_ok=True)\n",
        "\n",
        "# Sauvegarde du modèle\n",
        "joblib.dump(xgb_model, \"regression/xgboost_paddy_model.pkl\")\n",
        "\n",
        "# Chargement ultérieur\n",
        "loaded_model = joblib.load(\"regression/xgboost_paddy_model.pkl\")"
      ],
      "metadata": {
        "id": "UIy7Q9KlhuBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faire des prédictions\n",
        "y_pred = loaded_model.predict(X_test_scaled)\n",
        "print(\"Prédictions générées avec le modèle chargé :\", y_pred[:5])\n",
        "\n",
        "# Calcul des métriques\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Performance du modèle chargé sur le jeu de test :\")\n",
        "print(f\"RMSE : {rmse:.2f}\")\n",
        "print(f\"MAE  : {mae:.2f}\")\n",
        "print(f\"R²   : {r2:.3f}\")"
      ],
      "metadata": {
        "id": "F-Z24IB9iS2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Valeurs réelles et prédictions\n",
        "y_true = y_test.values\n",
        "y_pred_values = y_pred  # depuis le modèle chargé\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_true, y_pred_values, alpha=0.6, color='teal', edgecolor='k')\n",
        "plt.plot([y_true.min(), y_true.max()],\n",
        "         [y_true.min(), y_true.max()],\n",
        "         'r--', linewidth=2)  # ligne y = x\n",
        "\n",
        "plt.xlabel(\"Rendement réel (Kg)\")\n",
        "plt.ylabel(\"Rendement prédit (Kg)\")\n",
        "plt.title(\"Prédictions XGBoost vs Valeurs réelles\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lkT7USZdjC3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les points proches de la ligne rouge y = x montrent de bonnes prédictions.\n",
        "\n",
        "Les points qui s’éloignent indiquent des erreurs plus importantes."
      ],
      "metadata": {
        "id": "GQqvrIOBjWfW"
      }
    }
  ]
}