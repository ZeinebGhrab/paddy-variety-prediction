{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Probl√®me de Classification : Pr√©diction de la Vari√©t√© de Paddy (Riz)**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-B7Wp0PyFWB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìã D√©finition du Probl√®me**\n",
        "\n",
        "**1. Contexte Agricole**\n",
        "\n",
        "Dans la culture du paddy (riz) en Inde (districts comme Cuddalore, Kurinjipadi, etc.), le choix de la vari√©t√© de semence (ex. : CO_43, ponmani, delux ponni) est critique pour maximiser le rendement en fonction des conditions locales :\n",
        "\n",
        "- Facteurs influents : Type de sol (alluvial, clay), conditions m√©t√©orologiques (pluie par p√©riode : 30DRain, temp√©ratures min/max par phase de croissance, vent, humidit√©), pratiques culturales (Nursery: dry/wet, intrants comme DAP_20days, Urea_40Days, pesticides), bloc agricole (Agriblock), superficie (Hectares), et rendement observ√© (Paddy yield(in Kg)).\n",
        "- D√©fi r√©el : Les agriculteurs doivent s√©lectionner la vari√©t√© optimale avant plantation, bas√©e sur des donn√©es historiques/environnementales, pour optimiser rendement et r√©sistance (ex. : ponmani tol√®re mieux l'argile humide, CO_43 alluvial sec).\n",
        "\n",
        "**2. Probl√®me ML**\n",
        "\n",
        "Classification multi-classe pour pr√©dire la Vari√©t√© de Paddy ('CO_43', 'ponmani', 'delux ponni') √† partir des features agronomiques et m√©t√©o.\n",
        "\n",
        "- Objectif : Recommander la vari√©t√© la plus adapt√©e (r√©duire risques, ‚Üë rendement de 10-20% en moyenne).\n",
        "- Impact : Outil d√©cisionnel pour fermiers (ex. : app mobile input sol/pluie ‚Üí output vari√©t√©)."
      ],
      "metadata": {
        "id": "j3zzw-umF9MT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_h9ziAVjoiX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import skew, kurtosis\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score, roc_curve)\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score, confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "import joblib\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration des graphiques\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "sns.set_palette(\"husl\")"
      ],
      "metadata": {
        "id": "w029FBcTj6iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le fichier original\n",
        "df = pd.read_csv('data/paddy_dataset_fe.csv', sep=',', encoding='utf-8', low_memory=False)"
      ],
      "metadata": {
        "id": "lKVC1fT2j8t3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aper√ßu des premi√®res lignes\n",
        "print(\"APER√áU DES DONN√âES:\")\n",
        "print(\"-\" * 30)\n",
        "print(df.head())\n",
        "print()"
      ],
      "metadata": {
        "id": "1k8mmeNtkUra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le fichier original\n",
        "df = pd.read_csv('data/paddy_dataset_fe.csv', sep=',', encoding='utf-8', low_memory=False)\n",
        "# Cr√©ation d'une copie pour ne pas modifier df original\n",
        "df_class = df.copy()"
      ],
      "metadata": {
        "id": "33vfc-6SkV7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aper√ßu des premi√®res lignes\n",
        "print(\"APER√áU DES DONN√âES:\")\n",
        "print(\"-\" * 30)\n",
        "print(df_class.head())\n",
        "print()"
      ],
      "metadata": {
        "id": "PHvfMa9rkk0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# S√©paration X et y\n",
        "# S√©paration X et y (apr√®s One-Hot Encoding avec drop_first=True)\n",
        "\n",
        "# Identification automatique des colonnes One-Hot cr√©√©es pour Variety\n",
        "variety_onehot_cols = [col for col in df.columns if col.startswith('Variety_')]\n",
        "\n",
        "print(\"Colonnes One-Hot d√©tect√©es pour la cible 'Variety' :\")\n",
        "print(variety_onehot_cols)\n",
        "\n",
        "# S√©paration features / target\n",
        "X = df.drop(variety_onehot_cols, axis=1)   # Toutes les colonnes sauf les One-Hot de Variety\n",
        "y = df[variety_onehot_cols]                # y = les colonnes One-Hot (format multi-colonnes)\n",
        "\n",
        "print(f\"Shape de X: {X.shape}\")\n",
        "print(f\"Shape de y: {y.shape}\")\n",
        "print(f\"\\nNombre de features: {X.shape[1]}\")\n",
        "print(f\"\\nListe des features:\")\n",
        "print(X.columns.tolist()[:30])  # Affiche les 30 premi√®res"
      ],
      "metadata": {
        "id": "IQcej6EoknQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Split Train/Test"
      ],
      "metadata": {
        "id": "woDjXT0AiJXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split des donn√©es\n",
        "\n",
        "# Conversion one-hot (n,3) ‚Üí labels (n,)\n",
        "y_labels = np.argmax(y.values, axis=1)\n",
        "\n",
        "# Split stratifi√©\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y_labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_labels\n",
        ")\n",
        "\n",
        "print(\"Split des donn√©es (80% train - 20% test):\")\n",
        "print(\"=\"*80)\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape : {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape : {y_test.shape}\")\n",
        "\n",
        "# Noms des vari√©t√©s\n",
        "variety_names = [col.replace('Variety_', '') for col in y.columns]\n",
        "\n",
        "print(\"\\nDistribution de la variable cible (Variety):\")\n",
        "print(\"-\"*80)\n",
        "for i, variety in enumerate(variety_names):\n",
        "    train_count = np.sum(y_train == i)\n",
        "    test_count  = np.sum(y_test == i)\n",
        "\n",
        "    print(f\"Train - {variety:12} : {train_count:4d} ({train_count/len(y_train)*100:5.2f}%)\")\n",
        "    print(f\"Test  - {variety:12} : {test_count:4d} ({test_count/len(y_test)*100:5.2f}%)\")"
      ],
      "metadata": {
        "id": "T7uPD4Filtj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Normalisation des donn√©es"
      ],
      "metadata": {
        "id": "wTkGzdFkk55c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardisation des features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Normalisation effectu√©e avec StandardScaler\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nStatistiques apr√®s normalisation (X_train):\")\n",
        "print(f\"Moyenne: {X_train_scaled.mean(axis=0).mean():.6f}\")\n",
        "print(f\"√âcart-type: {X_train_scaled.std(axis=0).mean():.6f}\")\n",
        "\n",
        "# Conversion en DataFrame pour plus de clart√©\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns)"
      ],
      "metadata": {
        "id": "d9A-ZBVil73x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 3. Mod√©lisation & √âvaluation\n",
        "---"
      ],
      "metadata": {
        "id": "IwAW_PzzlBAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(name, model, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Fonction g√©n√©rique d'√©valuation (binaire & multi-classes)\n",
        "    \"\"\"\n",
        "\n",
        "    # Pr√©dictions\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "\n",
        "    n_classes = len(np.unique(y_test))\n",
        "\n",
        "    # ROC-AUC\n",
        "    roc_auc = None\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        y_pred_proba = model.predict_proba(X_test)\n",
        "\n",
        "        if n_classes == 2:\n",
        "            roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
        "        else:\n",
        "            roc_auc = roc_auc_score(\n",
        "                y_test,\n",
        "                y_pred_proba,\n",
        "                multi_class='ovr',\n",
        "                average='weighted'\n",
        "            )\n",
        "\n",
        "    # M√©triques\n",
        "    results = {\n",
        "        'Mod√®le': name,\n",
        "        'Accuracy Train': accuracy_score(y_train, y_pred_train),\n",
        "        'Accuracy Test': accuracy_score(y_test, y_pred_test),\n",
        "        'Precision': precision_score(y_test, y_pred_test, average='weighted'),\n",
        "        'Recall': recall_score(y_test, y_pred_test, average='weighted'),\n",
        "        'F1-Score': f1_score(y_test, y_pred_test, average='weighted'),\n",
        "        'ROC-AUC': roc_auc\n",
        "    }\n",
        "\n",
        "    # Affichage des r√©sultats\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"MOD√àLE: {name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for metric, value in results.items():\n",
        "        if metric != 'Mod√®le':\n",
        "            if value is not None:\n",
        "                print(f\"{metric:20s}: {value:.4f}\")\n",
        "            else:\n",
        "                print(f\"{metric:20s}: N/A\")\n",
        "\n",
        "    # Matrice de confusion\n",
        "    cm = confusion_matrix(y_test, y_pred_test)\n",
        "    class_labels = np.unique(y_test)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_labels,\n",
        "                yticklabels=class_labels)\n",
        "    plt.title(f'Matrice de Confusion - {name}', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Vraie Classe')\n",
        "    plt.xlabel('Classe Pr√©dite')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Rapport de classification\n",
        "    print(\"\\nRapport de classification:\")\n",
        "    print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "FAlf9zlyl_6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 K-Nearest Neighbors (KNN)"
      ],
      "metadata": {
        "id": "PoEYao02lLEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Entra√Ænement du mod√®le KNN\n",
        "print(\"Entra√Ænement du mod√®le K-Nearest Neighbors...\")\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "\n",
        "# √âvaluation\n",
        "results_knn = evaluate_model('K-Nearest Neighbors', knn,\n",
        "                             X_train_scaled, X_test_scaled,\n",
        "                             y_train, y_test)"
      ],
      "metadata": {
        "id": "iTKDq8Cemqv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Logistic Regression"
      ],
      "metadata": {
        "id": "JfSYeeXyFlwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entra√Ænement du mod√®le Logistic Regression\n",
        "print(\"Entra√Ænement du mod√®le Logistic Regression...\")\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "\n",
        "# √âvaluation\n",
        "results_lr = evaluate_model('Logistic Regression', lr,\n",
        "                            X_train_scaled, X_test_scaled,\n",
        "                            y_train, y_test)"
      ],
      "metadata": {
        "id": "7pQOhBXMmzjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Decision Tree avec visualisation"
      ],
      "metadata": {
        "id": "fVRol1vCFqia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entra√Ænement du mod√®le Decision Tree\n",
        "print(\"Entra√Ænement du mod√®le Decision Tree...\")\n",
        "dt = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "dt.fit(X_train_scaled, y_train)\n",
        "\n",
        "# √âvaluation\n",
        "results_dt = evaluate_model('Decision Tree', dt,\n",
        "                           X_train_scaled, X_test_scaled,\n",
        "                           y_train, y_test)"
      ],
      "metadata": {
        "id": "7F5JUDo2nfAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation de l'arbre de d√©cision\n",
        "plt.figure(figsize=(22, 12))\n",
        "\n",
        "plot_tree(\n",
        "    dt,                                  # le mod√®le entra√Æn√©\n",
        "    feature_names=X.columns,             # noms des features\n",
        "    class_names=dt.classes_.astype(str), # classes r√©elles\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    fontsize=9\n",
        ")\n",
        "\n",
        "plt.title(\"Visualisation de l'Arbre de D√©cision\", fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iiwSDTQtnmss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Random Forest avec Feature Importance"
      ],
      "metadata": {
        "id": "89aUS7z3F5zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entra√Ænement du mod√®le Random Forest\n",
        "print(\"Entra√Ænement du mod√®le Random Forest...\")\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# √âvaluation\n",
        "results_rf = evaluate_model('Random Forest', rf,\n",
        "                           X_train_scaled, X_test_scaled,\n",
        "                           y_train, y_test)"
      ],
      "metadata": {
        "id": "XECg2Dwtnq_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trier par importance d√©croissante\n",
        "feature_importance_sorted = feature_importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "# S√©lectionner les Top N features les plus importantes (par ex. top 5)\n",
        "top_n = 5\n",
        "top_features = feature_importance_sorted.head(top_n)\n",
        "\n",
        "# Visualisation avec barres horizontales\n",
        "plt.figure(figsize=(8, 6))\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, len(top_features)))\n",
        "plt.barh(top_features['Feature'][::-1], top_features['Importance'][::-1],\n",
        "         color=colors, edgecolor='black', alpha=0.8)\n",
        "plt.xlabel('Importance', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Features', fontsize=12, fontweight='bold')\n",
        "plt.title(f'Top {top_n} Feature Importance - Random Forest', fontsize=14, fontweight='bold')\n",
        "plt.grid(axis='x', alpha=0.8)\n",
        "\n",
        "# Ajouter les valeurs\n",
        "for i, v in enumerate(top_features['Importance'][::-1]):\n",
        "    plt.text(v + 0.001, i, f'{v:.4f}', va='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nTop {top_n} Features les plus importantes:\")\n",
        "print(top_features.to_string(index=False))"
      ],
      "metadata": {
        "id": "z9H2DFYrnt64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 XGBoost"
      ],
      "metadata": {
        "id": "xRbmSbf9GIcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entra√Ænement du mod√®le XGBoost\n",
        "print(\"Entra√Ænement du mod√®le XGBoost...\")\n",
        "xgb = XGBClassifier(n_estimators=100, random_state=42, n_jobs=-1, eval_metric='logloss')\n",
        "xgb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# √âvaluation\n",
        "results_xgb = evaluate_model('XGBoost', xgb,\n",
        "                            X_train_scaled, X_test_scaled,\n",
        "                            y_train, y_test)"
      ],
      "metadata": {
        "id": "uHTuTDXsn_ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.7 Comparaison des mod√®les baseline"
      ],
      "metadata": {
        "id": "2FdfeR3QGOjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparaison des r√©sultats\n",
        "comparison_df = pd.DataFrame([results_knn, results_lr, results_dt, results_rf, results_xgb])\n",
        "comparison_df = comparison_df.set_index('Mod√®le')\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"COMPARAISON DES MOD√àLES BASELINE\")\n",
        "print(\"=\"*100)\n",
        "print(comparison_df.to_string())\n",
        "\n",
        "# Visualisation comparative\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "metrics = ['Accuracy Test', 'Precision', 'Recall', 'F1-Score']\n",
        "colors_palette = sns.color_palette('husl', len(comparison_df))\n",
        "\n",
        "for idx, metric in enumerate(metrics):\n",
        "    row = idx // 2\n",
        "    col = idx % 2\n",
        "\n",
        "    comparison_df[metric].plot(kind='bar', ax=axes[row, col],\n",
        "                               color=colors_palette, alpha=0.8, edgecolor='black')\n",
        "    axes[row, col].set_title(f'Comparaison - {metric}', fontsize=13, fontweight='bold')\n",
        "    axes[row, col].set_ylabel('Score', fontsize=11)\n",
        "    axes[row, col].set_xlabel('')\n",
        "    axes[row, col].grid(axis='y', alpha=0.3)\n",
        "    axes[row, col].set_xticklabels(axes[row, col].get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "    # Ajouter les valeurs sur les barres\n",
        "    for i, v in enumerate(comparison_df[metric]):\n",
        "        axes[row, col].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2sXVM56zoHCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== MOD√àLES SANS PCA ===\")\n",
        "\n",
        "# ==============================\n",
        "# Decision Trees\n",
        "# ==============================\n",
        "dt_gini = DecisionTreeClassifier(\n",
        "    criterion='gini',\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "dt_entropy = DecisionTreeClassifier(\n",
        "    criterion='entropy',\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "dt_gini.fit(X_train_scaled, y_train)\n",
        "dt_entropy.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_dt_gini = dt_gini.predict(X_test_scaled)\n",
        "y_pred_dt_entropy = dt_entropy.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\nDecision Tree - Gini\")\n",
        "print(classification_report(y_test, y_pred_dt_gini))\n",
        "print(\"Accuracy :\", accuracy_score(y_test, y_pred_dt_gini))\n",
        "print(\"F1-score :\", f1_score(y_test, y_pred_dt_gini, average='weighted'))\n",
        "\n",
        "print(\"\\nDecision Tree - Entropy\")\n",
        "print(classification_report(y_test, y_pred_dt_entropy))\n",
        "print(\"Accuracy :\", accuracy_score(y_test, y_pred_dt_entropy))\n",
        "print(\"F1-score :\", f1_score(y_test, y_pred_dt_entropy, average='weighted'))\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# Random Forest\n",
        "# ==============================\n",
        "rf_gini = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    criterion='gini',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_entropy = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    criterion='entropy',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_gini.fit(X_train_scaled, y_train)\n",
        "rf_entropy.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_rf_gini = rf_gini.predict(X_test_scaled)\n",
        "y_pred_rf_entropy = rf_entropy.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\nRandom Forest - Gini\")\n",
        "print(classification_report(y_test, y_pred_rf_gini))\n",
        "print(\"Accuracy :\", accuracy_score(y_test, y_pred_rf_gini))\n",
        "print(\"F1-score :\", f1_score(y_test, y_pred_rf_gini, average='weighted'))\n",
        "\n",
        "print(\"\\nRandom Forest - Entropy\")\n",
        "print(classification_report(y_test, y_pred_rf_entropy))\n",
        "print(\"Accuracy :\", accuracy_score(y_test, y_pred_rf_entropy))\n",
        "print(\"F1-score :\", f1_score(y_test, y_pred_rf_entropy, average='weighted'))"
      ],
      "metadata": {
        "id": "4QwUwZqroXLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le but d‚Äôutiliser Gini ou Entropie dans les arbres de d√©cision et les for√™ts al√©atoires est le m√™me :\n",
        "\n",
        "`Trouver la meilleure s√©paration possible √† chaque n≈ìud pour que les sous-n≈ìuds soient les plus purs possibles.`\n",
        "\n",
        "Autrement dit :\n",
        "\n",
        "- On veut que chaque branche contienne des exemples aussi homog√®nes que possible (ex. tous de la m√™me classe).\n",
        "\n",
        "- Gini et Entropie sont juste deux fa√ßons diff√©rentes de mesurer l‚Äôimpuret√© ou le d√©sordre d‚Äôun n≈ìud.\n",
        "\n",
        "- L‚Äôarbre utilise cette mesure pour d√©cider quelle variable et quelle valeur de split choisir √† chaque √©tape."
      ],
      "metadata": {
        "id": "01dvXAaUPySc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 4. Fine-tuning des Meilleurs Mod√®les\n",
        "---"
      ],
      "metadata": {
        "id": "9RmnvbebGYzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 S√©lection des meilleurs mod√®les"
      ],
      "metadata": {
        "id": "Cy5dvaDuGb0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# S√©lection des 3 meilleurs mod√®les bas√©s sur le F1-Score\n",
        "best_models_idx = comparison_df['F1-Score'].nlargest(3).index\n",
        "print(\"Top 3 mod√®les s√©lectionn√©s pour le fine-tuning:\")\n",
        "print(\"=\"*80)\n",
        "for i, model_name in enumerate(best_models_idx, 1):\n",
        "    print(f\"{i}. {model_name} - F1-Score: {comparison_df.loc[model_name, 'F1-Score']:.4f}\")"
      ],
      "metadata": {
        "id": "Yym2pgOJoj3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Fine-tuning avec GridSearchCV"
      ],
      "metadata": {
        "id": "aXaIkuBTGgfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionnaire des mod√®les et param√®tres √† tuner\n",
        "models_to_tune = {\n",
        "    'Logistic Regression': {\n",
        "        'model': LogisticRegression(max_iter=1000, random_state=42),\n",
        "        'params': {\n",
        "            'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "            'penalty': ['l2'],\n",
        "            'solver': ['lbfgs', 'liblinear']\n",
        "        }\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'model': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [5, 10, 15, 20, None],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4]\n",
        "        }\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'model': XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss'),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [3, 5, 7, 9],\n",
        "            'learning_rate': [0.01, 0.1, 0.3],\n",
        "            'subsample': [0.8, 0.9, 1.0],\n",
        "            'colsample_bytree': [0.8, 0.9, 1.0]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Stockage des mod√®les tun√©s\n",
        "tuned_models = {}\n",
        "tuned_results = []\n",
        "\n",
        "for model_name, config in models_to_tune.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"FINE-TUNING: {model_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # GridSearchCV\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=config['model'],\n",
        "        param_grid=config['params'],\n",
        "        cv=5,\n",
        "        scoring='f1',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "    print(f\"\\nMeilleurs param√®tres trouv√©s:\")\n",
        "    for param, value in grid_search.best_params_.items():\n",
        "        print(f\"  {param}: {value}\")\n",
        "\n",
        "    print(f\"\\nMeilleur score (F1) en validation crois√©e: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "    # Stocker le meilleur mod√®le\n",
        "    tuned_models[model_name] = grid_search.best_estimator_\n",
        "\n",
        "    # √âvaluation sur le test set\n",
        "    results = evaluate_model(f'{model_name} (Tuned)',\n",
        "                            grid_search.best_estimator_,\n",
        "                            X_train_scaled, X_test_scaled,\n",
        "                            y_train, y_test)\n",
        "    tuned_results.append(results)"
      ],
      "metadata": {
        "id": "Kst37Xa1op8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Comparaison Avant/Apr√®s Fine-tuning"
      ],
      "metadata": {
        "id": "r9chc-ejGr1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparaison des r√©sultats\n",
        "tuned_comparison_df = pd.DataFrame(tuned_results).set_index('Mod√®le')\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"COMPARAISON DES MOD√àLES APR√àS FINE-TUNING\")\n",
        "print(\"=\"*100)\n",
        "print(tuned_comparison_df.to_string())\n",
        "\n",
        "# Visualisation comparative\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "x = np.arange(len(tuned_comparison_df))\n",
        "width = 0.15\n",
        "\n",
        "metrics = ['Accuracy Test', 'Precision', 'Recall', 'F1-Score']\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
        "\n",
        "for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
        "    ax.bar(x + i*width, tuned_comparison_df[metric], width,\n",
        "           label=metric, color=color, alpha=0.8, edgecolor='black')\n",
        "\n",
        "ax.set_xlabel('Mod√®les', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Comparaison des M√©triques - Mod√®les Tun√©s', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x + width * 1.5)\n",
        "ax.set_xticklabels(tuned_comparison_df.index, rotation=15, ha='right')\n",
        "ax.legend(loc='lower right', fontsize=10)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mgZlAHCuoxH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Performance g√©n√©rale :**\n",
        "\n",
        "- XGBoost a la meilleure accuracy, precision, recall, F1-score, et ROC-AUC, ce qui montre qu‚Äôil excelle √† la fois sur la d√©tection des classes et la g√©n√©ralisation.\n",
        "\n",
        "**Overfitting :**\n",
        "\n",
        "- Random Forest a 100% de pr√©cision sur le train, mais perd un peu en test (0.844), ce qui montre un l√©ger surapprentissage.\n",
        "\n",
        "- XGBoost reste tr√®s performant sur le test (0.950) tout en ayant une accuracy train l√©g√®rement inf√©rieure √† 1 (0.958), ce qui montre un bon √©quilibre.\n",
        "\n",
        "**ROC-AUC :**\n",
        "\n",
        "- XGBoost atteint 0.982, ce qui est excellent et indique qu‚Äôil s√©pare tr√®s bien les classes positives et n√©gatives.\n",
        "\n",
        "> Conclusion : Pour ce dataset, XGBoost (Tuned) est le mod√®le √† retenir pour les pr√©dictions."
      ],
      "metadata": {
        "id": "KwZzAELSuvML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Courbes ROC des mod√®les tun√©s"
      ],
      "metadata": {
        "id": "U_FL_UY-GvGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
        "\n",
        "# Convertir y_test en 1D si n√©cessaire\n",
        "if isinstance(y_test, np.ndarray) and y_test.ndim > 1 and y_test.shape[1] > 1:\n",
        "    y_test_labels = np.argmax(y_test, axis=1)  # convertit one-hot en labels 1D\n",
        "else:\n",
        "    y_test_labels = y_test.flatten()  # si d√©j√† 1D\n",
        "\n",
        "# Boucle pour chaque mod√®le tun√©\n",
        "for model_name, model in tuned_models.items():\n",
        "    y_pred_proba = model.predict_proba(X_test_scaled)\n",
        "\n",
        "    # ROC-AUC multi-classe (One-vs-Rest)\n",
        "    roc_auc = roc_auc_score(y_test_labels, y_pred_proba[:, 1])\n",
        "    print(f\"{model_name:25s}: ROC-AUC = {roc_auc:.4f}\")\n",
        "\n",
        "    # Tracer les courbes ROC\n",
        "    plt.figure(figsize=(8,5))\n",
        "    for i, color in zip(range(y_pred_proba.shape[1]), colors):\n",
        "        fpr, tpr, _ = roc_curve(y_test_labels == i, y_pred_proba[:, i])\n",
        "        plt.plot(fpr, tpr, color=color, lw=2.5, label=f'Classe {i}')\n",
        "\n",
        "    plt.plot([0,1], [0,1], 'k--', lw=2, label='Al√©atoire (AUC=0.5)')\n",
        "    plt.xlabel(\"FPR\")\n",
        "    plt.ylabel(\"TPR\")\n",
        "    plt.title(f\"ROC multi-classe - {model_name}\")\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "1QlBD4DEo3_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparaison des mod√®les (ROC-AUC multi-classe)\n",
        "- XGBoost : Courbes tr√®s hautes et proches du coin sup√©rieur gauche ‚Üí tr√®s bonne s√©paration des 3 classes.\n",
        "- Random Forest : Bonnes courbes, un peu moins nettes que XGBoost.\n",
        "- Logistic Regression : Courbes basses, surtout pour la classe rouge ‚Üí difficult√© √† s√©parer les classes (mod√®le trop simple)."
      ],
      "metadata": {
        "id": "Lqki2xJFo8I_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 5. S√©lection de Caract√©ristiques avec SelectKBest\n",
        "---"
      ],
      "metadata": {
        "id": "qeLpy_nGG2hq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 S√©lection des K meilleures features"
      ],
      "metadata": {
        "id": "EqIklGyKHBLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SelectKBest avec diff√©rentes valeurs de K\n",
        "k_values = [5, 7, 10]\n",
        "feature_selection_results = []\n",
        "\n",
        "for k in k_values:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"S√âLECTION DES {k} MEILLEURES FEATURES\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # SelectKBest\n",
        "    selector = SelectKBest(score_func=f_classif, k=k)\n",
        "    X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
        "    X_test_selected = selector.transform(X_test_scaled)\n",
        "\n",
        "    # Obtenir les noms des features s√©lectionn√©es\n",
        "    selected_features = X.columns[selector.get_support()].tolist()\n",
        "\n",
        "    print(f\"\\nFeatures s√©lectionn√©es ({k}):\")\n",
        "    for i, feature in enumerate(selected_features, 1):\n",
        "        print(f\"  {i}. {feature}\")\n",
        "\n",
        "    # Scores des features\n",
        "    feature_scores = pd.DataFrame({\n",
        "        'Feature': X.columns[selector.get_support()],\n",
        "        'Score': selector.scores_[selector.get_support()]\n",
        "    }).sort_values('Score', ascending=False)\n",
        "\n",
        "    print(f\"\\nScores des features:\")\n",
        "    print(feature_scores.to_string(index=False))\n",
        "\n",
        "    # Tester avec diff√©rents mod√®les\n",
        "    models_test = {\n",
        "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        'XGBoost': XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')\n",
        "    }\n",
        "\n",
        "    for model_name, model in models_test.items():\n",
        "        model.fit(X_train_selected, y_train)\n",
        "        y_pred = model.predict(X_test_selected)\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "        feature_selection_results.append({\n",
        "            'K': k,\n",
        "            'Mod√®le': model_name,\n",
        "            'Accuracy': accuracy,\n",
        "            'F1-Score': f1\n",
        "        })\n",
        "\n",
        "        print(f\"\\n{model_name} avec {k} features:\")\n",
        "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"  F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "-mfvUDdBpRmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**\n",
        "\n",
        "- Tr√®s stable : Accuracy = 0.7581 quelle que soit la s√©lection (5, 7 ou 10 features).\n",
        "\n",
        "- F1-Score stable √† ~0.654.\n",
        "\n",
        "- Cela montre que ce mod√®le est peu sensible √† l‚Äôajout de features suppl√©mentaires pour ce dataset.\n",
        "\n",
        "**Random Forest**\n",
        "\n",
        "- Performance chute quand on ajoute plus de features (10 features ‚Üí accuracy 0.6720).\n",
        "\n",
        "- Avec 7 features, F1-Score est le plus haut (0.6805), ce qui sugg√®re que certaines features suppl√©mentaires peuvent introduire du bruit pour ce mod√®le.\n",
        "\n",
        "**XGBoost**\n",
        "\n",
        "- Avec 5 features : accuracy 0.7545, F1 0.6520 ‚Üí proche de RF et LR.\n",
        "\n",
        "- Avec 7 features : l√©g√®re am√©lioration (accuracy 0.7186, F1 0.6608).\n",
        "\n",
        "- Avec 10 features : meilleure combinaison (accuracy 0.7276, F1 0.6726).\n",
        "\n",
        "- Indique que XGBoost b√©n√©ficie d‚Äôun peu plus de features, mais pas trop non plus."
      ],
      "metadata": {
        "id": "eWP1LdoCvUF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Top features selon l‚Äôimportance**\n",
        "\n",
        "- Wind Direction_D61_D90_sw ‚Üí Score 3.58\n",
        "\n",
        "- Wind Direction_D61_D90_se ‚Üí Score 2.43\n",
        "\n",
        "- Wind Direction_D91_D120_nw ‚Üí Score 1.98\n",
        "\n",
        "- Inst Wind Speed_D31_D60 ‚Üí Score 1.79\n",
        "\n",
        "- Max temp_D91_D120 ‚Üí Score 1.79\n",
        "\n",
        "> Ces features m√©t√©o semblent √™tre les plus influentes sur le rendement du paddy, ce qui est coh√©rent avec la logique agronomique (direction du vent et temp√©rature ont un impact sur le rendement)."
      ],
      "metadata": {
        "id": "WODbVPjMvoKJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Visualisation des scores avec SelectKBest"
      ],
      "metadata": {
        "id": "vjdYl4niHHgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualiser tous les scores des features\n",
        "# Limiter aux 10 features avec le score le plus √©lev√©\n",
        "top_n = 10\n",
        "selector_all = SelectKBest(score_func=f_classif, k='all')\n",
        "selector_all.fit(X_train_scaled, y_train)\n",
        "\n",
        "all_feature_scores = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Score': selector_all.scores_\n",
        "}).sort_values('Score', ascending=True)\n",
        "top_features = all_feature_scores.sort_values('Score', ascending=False).head(top_n)\n",
        "\n",
        "# Visualisation avec barres horizontales\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = plt.cm.plasma(np.linspace(0, 1, len(top_features)))\n",
        "plt.barh(top_features['Feature'][::-1], top_features['Score'][::-1],  # invers√© pour afficher la plus haute en haut\n",
        "         color=colors, edgecolor='black', alpha=0.8)\n",
        "plt.xlabel('Score F-value', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Features', fontsize=12, fontweight='bold')\n",
        "plt.title(f'Top {top_n} Features - SelectKBest (f_classif)', fontsize=14, fontweight='bold')\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Ajouter les valeurs sur les barres\n",
        "for i, v in enumerate(top_features['Score'][::-1]):\n",
        "    plt.text(v + 2, i, f'{v:.1f}', va='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p1kNSvgOpXG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Comparaison des performances avec diff√©rents K"
      ],
      "metadata": {
        "id": "chFUJ9T7HOap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparaison des r√©sultats\n",
        "fs_results_df = pd.DataFrame(feature_selection_results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"R√âSULTATS DE LA S√âLECTION DE FEATURES\")\n",
        "print(\"=\"*100)\n",
        "print(fs_results_df.to_string(index=False))\n",
        "\n",
        "# Visualisation\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Accuracy\n",
        "for model_name in fs_results_df['Mod√®le'].unique():\n",
        "    model_data = fs_results_df[fs_results_df['Mod√®le'] == model_name]\n",
        "    axes[0].plot(model_data['K'], model_data['Accuracy'],\n",
        "                marker='o', linewidth=2.5, markersize=10, label=model_name)\n",
        "\n",
        "axes[0].set_xlabel('Nombre de Features (K)', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('Accuracy vs Nombre de Features', fontsize=13, fontweight='bold')\n",
        "axes[0].legend(fontsize=10)\n",
        "axes[0].grid(alpha=0.3)\n",
        "axes[0].set_xticks(k_values)\n",
        "\n",
        "# F1-Score\n",
        "for model_name in fs_results_df['Mod√®le'].unique():\n",
        "    model_data = fs_results_df[fs_results_df['Mod√®le'] == model_name]\n",
        "    axes[1].plot(model_data['K'], model_data['F1-Score'],\n",
        "                marker='o', linewidth=2.5, markersize=10, label=model_name)\n",
        "\n",
        "axes[1].set_xlabel('Nombre de Features (K)', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
        "axes[1].set_title('F1-Score vs Nombre de Features', fontsize=13, fontweight='bold')\n",
        "axes[1].legend(fontsize=10)\n",
        "axes[1].grid(alpha=0.3)\n",
        "axes[1].set_xticks(k_values)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oHGpYBJcpeUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpr√©tation**\n",
        "\n",
        "1. Logistic Regression\n",
        "\n",
        "- Stable √† 0.758 d‚Äôaccuracy quelle que soit la taille K des features.\n",
        "\n",
        "- F1-Score stable aussi (~0.654).\n",
        "\n",
        "- Peu sensible √† l‚Äôajout de features suppl√©mentaires ‚Üí le mod√®le est simple et lin√©aire.\n",
        "\n",
        "2. Random Forest\n",
        "\n",
        "- F1-Score augmente l√©g√®rement avec 7 features (0.6805), mais baisse pour 10 features (0.6516).\n",
        "\n",
        "- Indique que certaines features suppl√©mentaires ajoutent du bruit et d√©gradent la performance.\n",
        "\n",
        "3. XGBoost\n",
        "\n",
        "- Performances am√©liorent avec 10 features (F1-Score 0.6726, Accuracy 0.7276) ‚Üí peut mieux g√©rer les interactions entre features.\n",
        "\n",
        "- Moins stable avec peu de features, mais gagne en complexit√© avec plus de variables."
      ],
      "metadata": {
        "id": "XXqXhxJywKQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cr√©er le dossier si n√©cessaire\n",
        "os.makedirs(\"classification\", exist_ok=True)\n",
        "\n",
        "# Trouver le meilleur mod√®le selon F1-Score\n",
        "best_result = max(feature_selection_results, key=lambda x: x['F1-Score'])\n",
        "best_k = best_result['K']\n",
        "best_model_name = best_result['Mod√®le']\n",
        "\n",
        "print(f\"Meilleur mod√®le : {best_model_name} avec K={best_k} features, F1-Score={best_result['F1-Score']:.4f}\")\n",
        "\n",
        "# Re-cr√©er et entra√Æner le mod√®le correspondant pour sauvegarde\n",
        "selector_best = SelectKBest(score_func=f_classif, k=best_k)\n",
        "X_train_best = selector_best.fit_transform(X_train_scaled, y_train)\n",
        "X_test_best = selector_best.transform(X_test_scaled)\n",
        "\n",
        "selected_features_best = X.columns[selector_best.get_support()].tolist()\n",
        "print(f\"Features s√©lectionn√©es pour le meilleur mod√®le : {selected_features_best}\")\n",
        "\n",
        "# Cr√©er le mod√®le\n",
        "if best_model_name == 'XGBoost':\n",
        "    model_best = XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')\n",
        "elif best_model_name == 'Random Forest':\n",
        "    model_best = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "else:\n",
        "    model_best = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Entra√Ænement\n",
        "model_best.fit(X_train_best, y_train)\n",
        "\n",
        "# Sauvegarde\n",
        "joblib.dump(model_best, f\"classification/best_model_{best_model_name}_K{best_k}.pkl\")\n",
        "print(f\"Mod√®le {best_model_name} sauvegard√© avec succ√®s !\")"
      ],
      "metadata": {
        "id": "ZVKeuvyLwda-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}