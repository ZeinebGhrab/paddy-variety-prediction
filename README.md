# ğŸŒ¾ Paddy Rice Yield Prediction & Farmer Profiling

Projet d'analyse de donnÃ©es agricoles combinant modÃ©lisation prÃ©dictive et segmentation pour optimiser la production rizicole en Inde.

---

## ğŸ“‹ Table des MatiÃ¨res

- [Vue d'ensemble](#-vue-densemble)  
- [Objectifs du Projet](#-objectifs-du-projet)  
- [DonnÃ©es](#-donnÃ©es)  
- [MÃ©thodologie](#-mÃ©thodologie)  
- [RÃ©sultats ClÃ©s](#-rÃ©sultats-clÃ©s)  
- [Structure du Projet](#-structure-du-projet)  
- [Installation](#-installation)  
- [Contribution](#-contribution)  
- [Licence](#-licence)  
- [Ã€ propos du dÃ©veloppeur](#-a-propos-du-developpeur)  

---

## ğŸ¯ Vue d'ensemble

Ce projet analyse 2 790 parcelles de riz en Inde pour :

- PrÃ©dire le rendement (kg/parcelle) Ã  partir des pratiques agricoles  
- Identifier 6 profils d'agriculteurs distincts via clustering  

Le rendement varie de 5 000 Ã  40 000 kg/parcelle, reflÃ©tant des stratÃ©gies de gestion trÃ¨s contrastÃ©es. L'objectif est de transformer ces donnÃ©es en leviers d'optimisation concrÃ¨te pour amÃ©liorer la productivitÃ© agricole.

---

## ğŸ¯ Objectifs du Projet

### 1ï¸âƒ£ ModÃ©lisation PrÃ©dictive (RÃ©gression)

- Construire un modÃ¨le pour prÃ©dire le rendement en riz (variable cible : `Paddy yield(in Kg)`)  
- Quantifier l'impact de chaque dÃ©cision agricole (engrais, pesticides, irrigation)  
- Identifier les variables les plus influentes sur la production  

### 2ï¸âƒ£ Segmentation des Agriculteurs (Clustering)

- DÃ©couvrir des profils agricoles homogÃ¨nes (intensif, optimal, Ã©conome...)  
- Extraire des recommandations personnalisÃ©es par profil  
- DÃ©tecter les anomalies (parcelles inefficaces malgrÃ© des intrants Ã©levÃ©s)  

---

## ğŸ“Š DonnÃ©es

**Source :** Dataset Paddy Rice (Inde) contenant 2 790 observations et 45 variables.  

| CatÃ©gorie       | Exemples de Variables                      |
|-----------------|-------------------------------------------|
| Intrants        | DAP, UrÃ©e, Potasse, Pesticides, Micronutriments |
| Pratiques       | Paille recyclÃ©e, DensitÃ© de semis, Surface cultivÃ©e |
| Environnement   | TempÃ©rature (min/max), PluviomÃ©trie (30j, 70j), Type de sol |
| Rendement       | Paddy yield(in Kg) â­ (variable cible)     |

**Fichiers de DonnÃ©es :**

```
data/
â”œâ”€â”€ paddydataset.csv
â”œâ”€â”€ noisy_paddydataset.csv
â”œâ”€â”€ cleaned_paddydataset.csv
â””â”€â”€ paddy_dataset_fe.csv
```


---

## ğŸ”¬ MÃ©thodologie

### Phase 1 : Exploration & Nettoyage
- EDA approfondie : distributions, corrÃ©lations, outliers  
- Nettoyage : gestion des valeurs manquantes, dÃ©tection d'anomalies  
- Feature Engineering : crÃ©ation de variables dÃ©rivÃ©es (ratios, agrÃ©gations temporelles)  

### Phase 2 : ModÃ©lisation RÃ©gression
**Feature Selection :**
- SelectKBest : sÃ©lection des 12 meilleures features  
- Backward Elimination : sÃ©lection basÃ©e sur p-values (OLS)  

**ModÃ¨les TestÃ©s :**
- RÃ©gression LinÃ©aire  
- Lasso (L1)  
- Ridge (L2)  
- ElasticNet (L1 + L2)  
- XGBoost Regressor â­  

**Ã‰valuation :**
- RMSE, MAE, RÂ²  
- Validation croisÃ©e (5-fold)  
- Analyse rÃ©siduelle  

### Phase 3 : Clustering
**RÃ©duction Dimensionnelle :**
- PCA : 47,5 % de variance expliquÃ©e avec 2 composantes  
- UMAP : prÃ©servation de la structure non-linÃ©aire  
- t-SNE : visualisation des clusters  

**Algorithmes :**
- K-Means (principal)  
- Clustering HiÃ©rarchique  
- GMM (Gaussian Mixture Model)  

**Optimisation du Nombre de Clusters :**
- Elbow Method + KneeLocator  
- Silhouette Score  
- BIC/AIC pour GMM  
- RÃ©sultat optimal : k=6  

**InterprÃ©tation :**
- Analyse des centroÃ¯des  
- Heatmap des profils  
- Decision Tree pour extraction de rÃ¨gles  

---

## ğŸ† RÃ©sultats ClÃ©s

### ğŸ“ˆ RÃ©gression : PrÃ©diction du Rendement

| ModÃ¨le          | RMSE Test | MAE Test | RÂ² Test | Verdict |
|-----------------|-----------|----------|---------|---------|
| Linear Regression | 3130.25  | 1652.84  | 0.8754  | Bon     |
| Lasso           | 3130.86  | 1652.96  | 0.8753  | Bon     |
| Ridge           | 3130.13  | 1652.65  | 0.8754  | Bon     |
| ElasticNet      | 3130.48  | 1652.72  | 0.8754  | Bon     |
| XGBoost         | 2938.82  | 1550.29  | 0.8830  | ğŸ¥‡ Meilleur |

**Variables les Plus Importantes :**
- TempÃ©rature maximale (J61-J90)  
- PluviomÃ©trie (70 jours)  
- DAP (engrais phosphatÃ©)  
- UrÃ©e (fertilisant azotÃ©)  
- Paille recyclÃ©e  

### ğŸ” Clustering : 6 Profils Agricoles IdentifiÃ©s

| Cluster | Profil         | Effectif | Rendement Moyen | CaractÃ©ristiques                    |
|---------|----------------|----------|----------------|-------------------------------------|
| 0       | ğŸ† Champion Intensif | 605      | 23 121 kg     | Intrants Ã©levÃ©s, paille maximale    |
| 5       | ğŸ¥ˆ Champion (variante) | 486      | 23 025 kg     | Similaire au Cluster 0              |
| 3       | â­ Profil Optimal | 425      | 22 619 kg     | Bon rendement, moins de ressources  |
| 1       | ğŸ“Š Standard     | 477      | 22 525 kg     | Pratiques moyennes                  |
| 4       | ğŸ“Š Standard     | 386      | 22 525 kg     | Pratiques moyennes                  |
| 2       | âš ï¸ Ã‰conome      | 410      | 22 409 kg     | Sous-investissement en intrants     |

**Score de Silhouette :**
- K-Means : 0.3449  
- Clustering HiÃ©rarchique : 0.3337  
- GMM : 0.3449  

---

## ğŸ“ Structure du Projet

```
paddy-variety-prediction/
â”‚
â”œâ”€â”€ data/                           # DonnÃ©es
â”‚   â”œâ”€â”€ paddydataset.csv            # Dataset original
â”‚   â”œâ”€â”€ noisy_paddydataset.csv      # Dataset avec bruit
â”‚   â”œâ”€â”€ paddy_dataset_fe.csv        # # Dataset aprÃ¨s ingÃ©nierie des caractÃ©ristiques
â”‚   â””â”€â”€ cleaned_paddydataset.csv    # Dataset nettoyÃ©
â”‚
â”‚
â”œâ”€â”€ src/                            # Scripts de traitement
â”‚   â”œâ”€â”€ 01_eda.py                   # Analyse exploratoire
â”‚   â”œâ”€â”€ 02_data_cleaning.py         # Nettoyage des donnÃ©es
â”‚   â”œâ”€â”€ 02_feature_engineering.py   # Engineering des features
â”‚   â”œâ”€â”€ 04_regression_modeling.py   # ModÃ©lisation rÃ©gression
â”‚   â””â”€â”€ 05_clustering_modeling.py   # ModÃ©lisation clustering
â”‚                    
â”œâ”€â”€ results/ 
â”‚   â”œâ”€â”€ cluster_assignments.scv     # Attribution des parcelles aux clusters   
â”‚   â”œâ”€â”€ cluster_centroids.csv       # Profils moyens de chaque cluster 
â”‚   â””â”€â”€ cluster_statistics.csv      # Statistiques dÃ©taillÃ©es par cluster
â”‚
â”œâ”€â”€ requirements.txt                # DÃ©pendances
â””â”€â”€ README.md                     

```


---

## ğŸ› ï¸ Installation

**PrÃ©requis :**
- Python 3.8+  
- pip  

**Ã‰tapes :**
```bash
# 1. Cloner le repository
git clone https://github.com/ZeinebGhrab/paddy-variety-prediction.git
cd paddy-variety-prediction
```

**2. CrÃ©er un environnement virtuel (recommandÃ©)**
```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```
**3. Installer les dÃ©pendances**
```bash
pip install -r requirements.txt
```
---

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Pour contribuer :

1. Forkez le projet
2. CrÃ©ez une branche (`git checkout -b feature/AmazingFeature`)
3. Committez vos changements (`git commit -m 'Add AmazingFeature'`)
4. Pushez vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrez une Pull Request

## ğŸ“„ Licence

MIT License Â© Zeineb Ghrab

## ğŸ™‹ Ã€ propos du dÃ©veloppeur  
RÃ©alisÃ©e avec passion par Zeineb Ghrab  
ğŸ“ IngÃ©nieure en Data Science | ğŸ§  PassionnÃ©e par les donnÃ©es, l'IA et le dÃ©veloppement full-stack
